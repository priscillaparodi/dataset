{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d24cd8-a437-4bd2-a1f0-93e535ccf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29119c-297d-44bf-b892-4d880bd91951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install eland elasticsearch sentence_transformers transformers torch==1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf90bc8-647e-4ada-9aa9-5cb9e60762b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from pathlib import Path\n",
    "from eland.ml.pytorch import PyTorchModel\n",
    "from eland.ml.pytorch.transformers import TransformerModel\n",
    "from elasticsearch.client import MlClient\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907a2bf-4927-428e-9ca8-9df3dd35a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found in the 'Manage Deployment' page\n",
    "CLOUD_ID = getpass.getpass('Enter Elastic Cloud ID:  ')\n",
    "\n",
    "# Password for the 'elastic' user generated by Elasticsearch\n",
    "ELASTIC_PASSWORD = getpass.getpass('Enter Elastic password:  ')\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    cloud_id=CLOUD_ID,\n",
    "    basic_auth=(\"elastic\", ELASTIC_PASSWORD),\n",
    "    request_timeout=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f3f5a-2b93-4a0c-93c8-c887ca80f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name from Hugging Face and task type\n",
    "# sentence-transformers model\n",
    "hf_model_id='sentence-transformers/all-mpnet-base-v2'\n",
    "tm = TransformerModel(hf_model_id, \"text_embedding\")\n",
    "\n",
    "#set the modelID as it is named in Elasticsearch\n",
    "es_model_id = tm.elasticsearch_model_id()\n",
    "\n",
    "# Download the model from Hugging Face\n",
    "tmp_path = \"models\"\n",
    "Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "model_path, config, vocab_path = tm.save(tmp_path)\n",
    "\n",
    "# Load the model into Elasticsearch\n",
    "ptm = PyTorchModel(client, es_model_id)\n",
    "ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)\n",
    "\n",
    "# Start the model\n",
    "s = MlClient.start_trained_model_deployment(client, model_id=es_model_id)\n",
    "s.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e115bd0-e758-44db-b5b9-96217af472c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.create(\n",
    "    index=\"ecommerce\",\n",
    "    mappings= {\n",
    "    \"properties\": {\n",
    "      \"product\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"description\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"category\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739f55b-6983-4b48-9349-6e0111b313fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an ingest pipeline with inference processors to use ELSER (sparse) and all-mpnet-base-v2 (dense) to infer against data that will be ingested in the pipeline.\n",
    "\n",
    "client.ingest.put_pipeline( \n",
    "    id=\"ecommerce-pipeline\",\n",
    "    processors = [\n",
    "    {\n",
    "      \"inference\": {\n",
    "        \"model_id\": \".elser_model_1\",\n",
    "        \"target_field\": \"ml\",\n",
    "        \"field_map\": {\n",
    "          \"description\": \"text_field\"\n",
    "        },\n",
    "        \"inference_config\": {\n",
    "          \"text_expansion\": { # text_expansion inference type (ELSER)\n",
    "            \"results_field\": \"tokens\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\n",
    "        \"target_field\": \"description_vector\", # Target field for the inference results\n",
    "        \"field_map\": {\n",
    "          \"description\": \"text_field\" # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53b39e-d74e-4fa8-a364-e2c3caf37418",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 'ecommerce-search'\n",
    "client.indices.create(\n",
    "            index=INDEX,\n",
    "            settings={\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 1,\n",
    "                    \"number_of_replicas\": 1\n",
    "                }\n",
    "            },\n",
    "            mappings={\n",
    "# Saving disk space by excluding the ELSER tokens and the dense_vector field from document source.\n",
    "# Note: That should only be applied if you are certain that reindexing will not be required in the future.\n",
    "            \"_source\" : {\n",
    "            \"excludes\": [\"ml.tokens\",\"description_vector.predicted_value\"]\n",
    "           }, \n",
    "        \"properties\": {\n",
    "        \"product\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"description\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"category\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"ml.tokens\": { # The name of the field to contain the generated tokens.\n",
    "        \"type\": \"rank_features\" # ELSER output must be ingested into a field with the rank_features field type.\n",
    "      }, \n",
    "     \"description_vector.predicted_value\": { # Inference results field, target_field.predicted_value\n",
    "     \"type\": \"dense_vector\", \n",
    "     \"dims\": 768, # The all-mpnet-base-v2 model has embedding_size of 768, so dims is set to 768.\n",
    "     \"index\": \"true\", \n",
    "     \"similarity\": \"dot_product\" #  When indexing vectors for approximate kNN search, you need to specify the similarity function for comparing the vectors.\n",
    " }\n",
    "  }\n",
    " \n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdc3b7-7e4f-4111-997b-c333ac8938ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecommerce dataset\n",
    "url = \"https://raw.githubusercontent.com/priscillaparodi/dataset/main/products-ecommerce.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "\n",
    "# Load the response data into a JSON object\n",
    "data_json = json.loads(response.read())\n",
    "\n",
    "def create_index_body(doc):\n",
    "    \"\"\" Generate the body for an Elasticsearch document. \"\"\"\n",
    "    return {\n",
    "        \"_index\": \"ecommerce\",\n",
    "        \"_source\": doc,\n",
    "    }\n",
    "\n",
    "# Prepare the documents to be indexed\n",
    "documents = [create_index_body(doc) for doc in data_json]\n",
    "\n",
    "# Use helpers.bulk to index\n",
    "helpers.bulk(client, documents)\n",
    "\n",
    "print(\"Done indexing documents into `ecommerce` index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297cb0b-ae2e-44f9-811d-27a41c43a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex data from one index 'source' to another 'dest' with the 'ecommerce-pipeline' pipeline.\n",
    "\n",
    "client.reindex(wait_for_completion=True,\n",
    "               source={\n",
    "                  \"index\": \"ecommerce\"\n",
    "    },\n",
    "               dest= {\n",
    "                  \"index\": \"ecommerce-search\",\n",
    "                  \"pipeline\": \"ecommerce-pipeline\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab4628-b611-4a6d-884e-4d2332066bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of results\n",
    "\n",
    "# BM25\n",
    "\n",
    "print(f\"\\nBM25:\\n\")\n",
    "\n",
    "response1 = client.search(size=3,\n",
    "    index=\"ecommerce-search\",\n",
    "    query= {\n",
    "            \"match\": {\n",
    "                    \"description\" : {  \n",
    "                    \"query\": \"<text>\"\n",
    "                        }\n",
    "                    }\n",
    "        }\n",
    ")\n",
    "hits = response1['hits']['hits']\n",
    "\n",
    "if not hits:\n",
    "    print(\"BM25 Result: No matches found\")\n",
    "else:\n",
    "    for hit in hits:\n",
    "        score = hit['_score']\n",
    "        product = hit['_source']['product']\n",
    "        category = hit['_source']['category']\n",
    "        description = hit['_source']['description']\n",
    "        print(f\"\\nScore: {score}\\nProduct: {product}\\nCategory: {category}\\nDescription: {description}\\n\")\n",
    "\n",
    "# KNN\n",
    "\n",
    "print(f\"\\nKNN:\\n\")\n",
    "\n",
    "response2 = client.search(index='ecommerce-search', size=3,\n",
    "            knn={\n",
    "            \"field\": \"description_vector.predicted_value\",\n",
    "            \"k\": 49,\n",
    "            \"num_candidates\": 2495,\n",
    "            \"query_vector_builder\": {\n",
    "            \"text_embedding\": { \n",
    "            \"model_id\": \"sentence-transformers__all-mpnet-base-v2\", \n",
    "            \"model_text\": \"<text>\" \n",
    "      }\n",
    "    }\n",
    "            }\n",
    ")\n",
    "\n",
    "for hit in response2['hits']['hits']:\n",
    "    \n",
    "    score = hit['_score']\n",
    "    product = hit['_source']['product']\n",
    "    category = hit['_source']['category']\n",
    "    description = hit['_source']['description']\n",
    "    print(f\"\\nScore: {score}\\nProduct: {product}\\nCategory: {category}\\nDescription: {description}\\n\")\n",
    "\n",
    "# ELSER\n",
    "\n",
    "print(f\"\\nELSER:\\n\")\n",
    "\n",
    "response3 = client.search(index='ecommerce-search', size=3,\n",
    "              query={\n",
    "                  \"text_expansion\": {\n",
    "                  \"ml.tokens\": {\n",
    "                      \"model_id\":\".elser_model_1\",\n",
    "                      \"model_text\":\"<text>\"                \n",
    "        }\n",
    "    }\n",
    "}\n",
    ")\n",
    "\n",
    "for hit in response3['hits']['hits']:\n",
    "\n",
    "    score = hit['_score']\n",
    "    product = hit['_source']['product']\n",
    "    category = hit['_source']['category']\n",
    "    description = hit['_source']['description']\n",
    "    print(f\"\\nScore: {score}\\nProduct: {product}\\nCategory: {category}\\nDescription: {description}\\n\")\n",
    "\n",
    "print(f\"\\nHybrid Search\\n\")\n",
    "\n",
    "# BM25 + KNN\n",
    "\n",
    "print(f\"\\nBM25 + KNN:\\n\")\n",
    "\n",
    "response4 = client.search(index='ecommerce-search', size=3,\n",
    "              query={\n",
    "             \"match\": {\n",
    "             \"description\" : {  \n",
    "             \"query\": \"<text>\",\n",
    "              \"boost\": 1\n",
    "                        }\n",
    "                        }                 \n",
    "                   \n",
    "},\n",
    "            knn={\n",
    "            \"field\": \"description_vector.predicted_value\",\n",
    "            \"k\": 49,\n",
    "            \"num_candidates\": 2495,\n",
    "            \"boost\": 1,\n",
    "            \"query_vector_builder\": {\n",
    "            \"text_embedding\": { \n",
    "            \"model_id\": \"sentence-transformers__all-mpnet-base-v2\", \n",
    "            \"model_text\": \"<text>\" \n",
    "      }\n",
    "    }\n",
    "            }\n",
    ")\n",
    "\n",
    "for hit in response4['hits']['hits']:\n",
    "\n",
    "    score = hit['_score']\n",
    "    product = hit['_source']['product']\n",
    "    category = hit['_source']['category']\n",
    "    description = hit['_source']['description']\n",
    "    print(f\"\\nScore: {score}\\nProduct: {product}\\nCategory: {category}\\nDescription: {description}\\n\")\n",
    "\n",
    "# BM25 + KNN (RRF)\n",
    "\n",
    "print(f\"\\nBM25 + KNN (RRF):\\n\")\n",
    "\n",
    "response5 = client.search(index='ecommerce-search', size=3,\n",
    "             query = {\n",
    "             \"match\": {\n",
    "             \"description\" : {  \n",
    "             \"query\": \"<text>\",\n",
    "                        }\n",
    "                        }                 \n",
    "                   \n",
    "},\n",
    "             knn = {\n",
    "            \"field\": \"description_vector.predicted_value\",\n",
    "            \"k\": 49,\n",
    "            \"num_candidates\": 2495,\n",
    "            \"query_vector_builder\": {\n",
    "            \"text_embedding\": { \n",
    "            \"model_id\": \"sentence-transformers__all-mpnet-base-v2\", \n",
    "            \"model_text\": \"<text>\" \n",
    "      }\n",
    "    }\n",
    "            },\n",
    "        rank = {\n",
    "        \"rrf\": {\n",
    "            \"window_size\": 2495,\n",
    "            \"rank_constant\": 10\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "for hit in response5['hits']['hits']:\n",
    "    \n",
    "    rank = hit['_rank']\n",
    "    category = hit['_source']['category']\n",
    "    product = hit['_source']['product']\n",
    "    description = hit['_source']['description']\n",
    "    print(f\"\\nRank: {rank}\\nProduct: {product}\\nCategory: {category}\\nDescription: {description}\\n\")\n",
    "\n",
    "# BM25 + ELSER\n",
    "# Note: Client is not compatible with Elastic 8.9 to use sub_searches parameter. Not adding RRF, for now.\n",
    "# Using compound query - 'should' appear in the matching document.\n",
    "\n",
    "print(f\"\\nBM25 + ELSER:\\n\")\n",
    "\n",
    "response6 = client.search(index='ecommerce-search', size=3,\n",
    "\n",
    "        query= {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"description\" : {  \n",
    "                            \"query\": \"<text>\",\n",
    "                            \"boost\": 1\n",
    "                        }\n",
    "                        }\n",
    "                    },                   \n",
    "                    {\n",
    "                        \"text_expansion\": {\n",
    "                            \"ml.tokens\": {\n",
    "                                \"model_id\": \".elser_model_1\",\n",
    "                                \"model_text\": \"<text>\",\n",
    "                                \"boost\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    ")\n",
    "\n",
    "for hit in response6['hits']['hits']:\n",
    "\n",
    "    score = hit['_score']\n",
    "    product = hit['_source']['product']\n",
    "    category = hit['_source']['category']\n",
    "    description = hit['_source']['description']\n",
    "    print(f\"\\nScore: {score}\\nProduct: {product}\\nCategory: {category}\\nDescription: {description}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72187c9a-14c1-4084-a080-4e5c1e614f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
